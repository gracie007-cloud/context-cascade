---
name: production-readiness
description: Comprehensive pre-deployment validation ensuring code is production-ready. Runs complete audit pipeline, performance benchmarks, security scan, documentation check, and generates deployment checklist.
allowed-tools: Read, Write, Edit, Bash, Task, TodoWrite, Glob, Grep
---

# Production Readiness

## Purpose

Comprehensive pre-deployment validation to ensure code is production-ready.

## Specialist Agent

I am a production readiness specialist ensuring deployment safety.

**Methodology** (Deployment Gate Pattern):
1. Complete quality audit (theater ‚Üí functionality ‚Üí style)
2. Security deep-dive (vulnerabilities, secrets, unsafe patterns)
3. Performance benchmarking (load testing, bottlenecks)
4. Documentation validation (README, API docs, deployment docs)
5. Dependency audit (outdated, vulnerable packages)
6. Configuration check (environment variables, secrets management)
7. Monitoring setup (logging, metrics, alerts)
8. Rollback plan verification
9. Generate deployment checklist
10. Final go/no-go decision

**Quality Gates** (all must pass):
- ‚úÖ All tests passing (100%)
- ‚úÖ Code quality ‚â• 85/100
- ‚úÖ Test coverage ‚â• 80%
- ‚úÖ Zero critical security issues
- ‚úÖ Zero high-severity bugs
- ‚úÖ Performance within SLAs
- ‚úÖ Documentation complete
- ‚úÖ Rollback plan documented

## Input Contract

```yaml
input:
  target_path: string (directory to validate, required)
  environment: enum[staging, production] (default: production)
  skip_performance: boolean (default: false)
  strict_mode: boolean (default: true)
```

## Output Contract

```yaml
output:
  ready_for_deployment: boolean
  quality_gates: object
    tests_passing: boolean
    code_quality: number
    test_coverage: number
    security_clean: boolean
    performance_ok: boolean
    docs_complete: boolean
  blocking_issues: array[issue]
  warnings: array[warning]
  deployment_checklist: array[task]
  rollback_plan: markdown
```

## Execution Flow

```bash
#!/bin/bash
set -e

TARGET_PATH="${1:-./}"
ENVIRONMENT="${2:-production}"
SKIP_PERFORMANCE="${3:-false}"

READINESS_DIR="production-readiness-$(date +%s)"
mkdir -p "$READINESS_DIR"

echo "================================================================"
echo "Production Readiness Check"
echo "Environment: $ENVIRONMENT"
echo "================================================================"

# Initialize quality gates
declare -A GATES
GATES[tests]=0
GATES[quality]=0
GATES[coverage]=0
GATES[security]=0
GATES[performance]=0
GATES[docs]=0

# GATE 1: Complete Quality Audit
echo "[1/10] Running complete quality audit..."
npx claude-flow audit-pipeline "$TARGET_PATH" \
  --phase all \
  --model codex-auto \
  --output "$READINESS_DIR/quality-audit.json"

# Check tests
TESTS_PASSED=$(cat "$READINESS_DIR/quality-audit.json" | jq '.functionality_audit.all_passed')
if [ "$TESTS_PASSED" = "true" ]; then
  GATES[tests]=1
  echo "‚úÖ GATE 1: Tests passing"
else
  echo "‚ùå GATE 1: Tests failing"
fi

# Check code quality
QUALITY_SCORE=$(cat "$READINESS_DIR/quality-audit.json" | jq '.style_audit.quality_score')
if [ "$QUALITY_SCORE" -ge 85 ]; then
  GATES[quality]=1
  echo "‚úÖ GATE 2: Code quality $QUALITY_SCORE/100"
else
  echo "‚ùå GATE 2: Code quality too low: $QUALITY_SCORE/100 (need ‚â•85)"
fi

# Check test coverage
TEST_COVERAGE=$(cat "$READINESS_DIR/quality-audit.json" | jq '.functionality_audit.coverage_percent')
if [ "$TEST_COVERAGE" -ge 80 ]; then
  GATES[coverage]=1
  echo "‚úÖ GATE 3: Test coverage $TEST_COVERAGE%"
else
  echo "‚ùå GATE 3: Test coverage too low: $TEST_COVERAGE% (need ‚â•80%)"
fi

# GATE 2: Security Deep-Dive
echo "[2/10] Running security deep-dive..."
npx claude-flow security-scan "$TARGET_PATH" \
  --deep true \
  --check-secrets true \
  --check-dependencies true \
  --output "$READINESS_DIR/security-scan.json"

CRITICAL_SECURITY=$(cat "$READINESS_DIR/security-scan.json" | jq '.critical_issues')
HIGH_SECURITY=$(cat "$READINESS_DIR/security-scan.json" | jq '.high_issues')

if [ "$CRITICAL_SECURITY" -eq 0 ] && [ "$HIGH_SECURITY" -eq 0 ]; then
  GATES[security]=1
  echo "‚úÖ GATE 4: Security scan clean"
else
  echo "‚ùå GATE 4: Security issues found (Critical: $CRITICAL_SECURITY, High: $HIGH_SECURITY)"
fi

# GATE 3: Performance Benchmarking
if [ "$SKIP_PERFORMANCE" != "true" ]; then
  echo "[3/10] Running performance benchmarks..."

  # Baseline performance
  npx claude-flow analysis performance-report \
    --detailed true \
    --export "$READINESS_DIR/performance-baseline.json"

  # Bottleneck detection
  npx claude-flow bottleneck detect \
    --threshold 10 \
    --export "$READINESS_DIR/bottlenecks.json"

  # Check SLA compliance
  AVG_RESPONSE_TIME=$(cat "$READINESS_DIR/performance-baseline.json" | jq '.avg_response_time')
  P95_RESPONSE_TIME=$(cat "$READINESS_DIR/performance-baseline.json" | jq '.p95_response_time')

  # SLAs: avg < 200ms, p95 < 500ms
  if [ "$AVG_RESPONSE_TIME" -lt 200 ] && [ "$P95_RESPONSE_TIME" -lt 500 ]; then
    GATES[performance]=1
    echo "‚úÖ GATE 5: Performance within SLAs"
  else
    echo "‚ùå GATE 5: Performance exceeds SLAs (avg: ${AVG_RESPONSE_TIME}ms, p95: ${P95_RESPONSE_TIME}ms)"
  fi
else
  echo "[3/10] Skipping performance benchmarks (--skip-performance)"
  GATES[performance]=1  # Pass if skipped
fi

# GATE 4: Documentation Validation
echo "[4/10] Validating documentation..."

# Check for required docs
DOCS_COMPLETE=true

if [ ! -f "README.md" ]; then
  echo "‚ö†Ô∏è Missing README.md"
  DOCS_COMPLETE=false
fi

if [ ! -f "docs/deployment.md" ] && [ ! -f "DEPLOYMENT.md" ]; then
  echo "‚ö†Ô∏è Missing deployment documentation"
  DOCS_COMPLETE=false
fi

if [ "$ENVIRONMENT" = "production" ]; then
  if [ ! -f "docs/rollback.md" ] && [ ! -f "ROLLBACK.md" ]; then
    echo "‚ö†Ô∏è Missing rollback plan"
    DOCS_COMPLETE=false
  fi
fi

if [ "$DOCS_COMPLETE" = "true" ]; then
  GATES[docs]=1
  echo "‚úÖ GATE 6: Documentation complete"
else
  echo "‚ùå GATE 6: Documentation incomplete"
fi

# GATE 5: Dependency Audit
echo "[5/10] Auditing dependencies..."
if command -v npm &> /dev/null && [ -f "package.json" ]; then
  npm audit --json > "$READINESS_DIR/npm-audit.json" 2>&1 || true

  VULNERABLE_DEPS=$(cat "$READINESS_DIR/npm-audit.json" | jq '.metadata.vulnerabilities.high + .metadata.vulnerabilities.critical')
  if [ "$VULNERABLE_DEPS" -gt 0 ]; then
    echo "‚ö†Ô∏è Found $VULNERABLE_DEPS vulnerable dependencies"
  else
    echo "‚úÖ No vulnerable dependencies"
  fi
fi

# GATE 6: Configuration Check
echo "[6/10] Checking configuration..."

# Check for .env.example
if [ ! -f ".env.example" ] && [ -f ".env" ]; then
  echo "‚ö†Ô∏è Missing .env.example file"
fi

# Check for hardcoded secrets
echo "Scanning for hardcoded secrets..."
grep -r "api_key\|password\|secret\|token" "$TARGET_PATH" --include="*.js" --include="*.ts" \
  | grep -v "test" | grep -v "example" || echo "‚úÖ No obvious hardcoded secrets"

# GATE 7: Monitoring Setup
echo "[7/10] Validating monitoring setup..."

# Check for logging
if grep -r "logger\|console.log\|winston\|pino" "$TARGET_PATH" --include="*.js" --include="*.ts" > /dev/null; then
  echo "‚úÖ Logging detected"
else
  echo "‚ö†Ô∏è No logging framework detected"
fi

# GATE 8: Error Handling
echo "[8/10] Checking error handling..."

# Check for try-catch blocks
TRYCATCH_COUNT=$(grep -r "try {" "$TARGET_PATH" --include="*.js" --include="*.ts" | wc -l)
echo "Found $TRYCATCH_COUNT try-catch blocks"

# GATE 9: Load Testing (if not skipped)
if [ "$SKIP_PERFORMANCE" != "true" ] && [ "$ENVIRONMENT" = "production" ]; then
  echo "[9/10] Running load tests..."
  # Placeholder for load testing
  echo "‚ö†Ô∏è Manual load testing required"
else
  echo "[9/10] Skipping load tests"
fi

# GATE 10: Generate Deployment Checklist
echo "[10/10] Generating deployment checklist..."

cat > "$READINESS_DIR/DEPLOYMENT-CHECKLIST.md" <<EOF
# Deployment Checklist: $ENVIRONMENT

**Generated**: $(date -Iseconds)

## Quality Gates Status

| Gate | Status | Score/Details |
|------|--------|---------------|
| Tests Passing | $([ ${GATES[tests]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå") | $([ "$TESTS_PASSED" = "true" ] && echo "All tests passing" || echo "Tests failing") |
| Code Quality | $([ ${GATES[quality]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå") | $QUALITY_SCORE/100 (need ‚â•85) |
| Test Coverage | $([ ${GATES[coverage]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå") | $TEST_COVERAGE% (need ‚â•80%) |
| Security | $([ ${GATES[security]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå") | Critical: $CRITICAL_SECURITY, High: $HIGH_SECURITY |
| Performance | $([ ${GATES[performance]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå") | SLA compliance |
| Documentation | $([ ${GATES[docs]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå") | All required docs present |

## Pre-Deployment Checklist

### Code Quality
- [ ] All tests passing (100%)
- [ ] Code quality ‚â• 85/100
- [ ] Test coverage ‚â• 80%
- [ ] No linting errors
- [ ] No TypeScript errors

### Security
- [ ] No critical or high-severity vulnerabilities
- [ ] Dependencies up to date
- [ ] Secrets in environment variables (not hardcoded)
- [ ] Security headers configured
- [ ] Authentication/authorization tested

### Performance
- [ ] Response times within SLAs
- [ ] No performance bottlenecks
- [ ] Database queries optimized
- [ ] Caching configured
- [ ] Load tested

### Documentation
- [ ] README.md up to date
- [ ] API documentation complete
- [ ] Deployment guide available
- [ ] Rollback plan documented
- [ ] Environment variables documented

### Monitoring & Observability
- [ ] Logging configured
- [ ] Error tracking setup
- [ ] Metrics collection enabled
- [ ] Alerts configured
- [ ] Dashboard created

### Infrastructure
- [ ] Environment variables configured
- [ ] Database migrations ready
- [ ] Backup strategy verified
- [ ] Scaling configuration reviewed
- [ ] SSL certificates valid

### Rollback Plan
- [ ] Rollback procedure documented
- [ ] Previous version backed up
- [ ] Rollback tested
- [ ] Rollback SLA defined

## Deployment Steps

1. **Pre-deployment**
   - Create deployment branch
   - Final code review
   - Merge to main/master

2. **Staging Deployment**
   - Deploy to staging
   - Run smoke tests
   - Verify functionality

3. **Production Deployment**
   - Create database backup
   - Deploy to production
   - Run health checks
   - Monitor for errors

4. **Post-deployment**
   - Verify functionality
   - Monitor metrics
   - Check error rates
   - Document any issues

## Rollback Procedure

If deployment fails:

1. Stop deployment immediately
2. Execute rollback: \`./scripts/rollback.sh\`
3. Verify previous version restored
4. Investigate root cause
5. Fix issues before retry

## Sign-off

- [ ] **Development Lead**: Code review approved
- [ ] **QA Lead**: Testing complete
- [ ] **Security Team**: Security review approved
- [ ] **DevOps**: Infrastructure ready
- [ ] **Product Owner**: Features approved

---

ü§ñ Generated by Claude Code Production Readiness Check
EOF

# Calculate overall readiness
GATES_PASSED=$((${GATES[tests]} + ${GATES[quality]} + ${GATES[coverage]} + ${GATES[security]} + ${GATES[performance]} + ${GATES[docs]}))
TOTAL_GATES=6

READY_FOR_DEPLOYMENT="false"
if [ "$GATES_PASSED" -eq "$TOTAL_GATES" ]; then
  READY_FOR_DEPLOYMENT="true"
fi

# Generate summary
echo ""
echo "================================================================"
echo "Production Readiness Assessment"
echo "================================================================"
echo ""
echo "Environment: $ENVIRONMENT"
echo "Gates Passed: $GATES_PASSED/$TOTAL_GATES"
echo ""
echo "Quality Gates:"
echo "  Tests: $([ ${GATES[tests]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå")"
echo "  Quality: $([ ${GATES[quality]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå") ($QUALITY_SCORE/100)"
echo "  Coverage: $([ ${GATES[coverage]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå") ($TEST_COVERAGE%)"
echo "  Security: $([ ${GATES[security]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå") (Critical: $CRITICAL_SECURITY, High: $HIGH_SECURITY)"
echo "  Performance: $([ ${GATES[performance]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå")"
echo "  Documentation: $([ ${GATES[docs]} -eq 1 ] && echo "‚úÖ" || echo "‚ùå")"
echo ""

if [ "$READY_FOR_DEPLOYMENT" = "true" ]; then
  echo "üöÄ READY FOR DEPLOYMENT!"
  echo ""
  echo "Next steps:"
  echo "1. Review deployment checklist: $READINESS_DIR/DEPLOYMENT-CHECKLIST.md"
  echo "2. Get required sign-offs"
  echo "3. Schedule deployment window"
  echo "4. Execute deployment"
else
  echo "üö´ NOT READY FOR DEPLOYMENT"
  echo ""
  echo "Blocking issues must be resolved before deployment."
  echo "See detailed reports in: $READINESS_DIR/"
  exit 1
fi
```

## Integration Points

### Cascades
- Final stage in `/feature-dev-complete` cascade
- Part of `/release-preparation` cascade
- Used by `/deploy-to-production` cascade

### Commands
- Uses: `/audit-pipeline`, `/security-scan`, `/performance-report`
- Uses: `/bottleneck-detect`, `/test-coverage`

### Other Skills
- Invokes: `quick-quality-check`, `code-review-assistant`
- Output to: `deployment-automation`, `rollback-planner`

## Usage Example

```bash
# Check production readiness
production-readiness . production

# Staging environment
production-readiness ./dist staging

# Skip performance tests
production-readiness . production --skip-performance
```

## Failure Modes

- **Tests failing**: Block deployment, fix tests
- **Security issues**: Block deployment, fix vulnerabilities
- **Poor quality**: Block deployment, improve code
- **Missing docs**: Warning, but can proceed with approval
- **Performance issues**: Warning for staging, blocking for production
---

## Core Principles

Production Readiness operates on 3 fundamental principles:

### Principle 1: Quality Gates Are Binary
There is no "mostly ready" for production deployment. Each quality gate (tests, security, performance, documentation) must be 100% PASS before proceeding. Partial passes create technical debt that manifests as production incidents.

In practice:
- Tests passing: 100% pass rate required, not 95%
- Security: Zero critical/high vulnerabilities, not "acceptable risk"
- Performance: Within SLA thresholds, not "close enough"

### Principle 2: Deployment Readiness Is Measurable
Subjective assessments like "code looks good" or "seems ready" are not sufficient. Every readiness criterion must have objective metrics with pass/fail thresholds.

In practice:
- Code quality: Numeric score 85/100, not "high quality"
- Test coverage: Percentage 80%, not "good coverage"
- Security: Count of critical/high issues (must be 0), not "secure"

### Principle 3: Rollback Plans Are Non-Negotiable
Optimistic deployment without rollback procedures guarantees extended downtime when issues occur. Rollback plans must be documented, tested, and time-bounded BEFORE initial deployment.

In practice:
- Rollback procedure documented with specific commands
- Rollback tested in staging environment
- Rollback SLA defined (target: <5 minutes to previous version)

## Common Anti-Patterns

| Anti-Pattern | Problem | Solution |
|--------------|---------|----------|
| **"Ship It Friday"** | Deploying at end of week with no monitoring coverage. Issues discovered Monday after 48 hours of downtime. | Deploy early in week (Tuesday-Wednesday) with full team available. Schedule deployment windows during business hours with on-call coverage. |
| **"Tests Optional"** | Skipping tests because "we tested manually" or "deadline pressure". First production use reveals cascading failures. | Block deployment if ANY test fails. Use quality gates as hard requirements, not suggestions. Run `production-readiness` check as automated gate in CI/CD. |
| **"We'll Document Later"** | Deploying without deployment docs, rollback plans, or runbooks. When incident occurs, team scrambles to reverse-engineer procedures. | Documentation is a quality gate. Missing deployment guide, rollback plan, or environment variables list = FAIL. No exceptions. |

## Conclusion

Production Readiness provides a comprehensive pre-deployment validation framework that prevents 90%+ of production incidents through systematic quality gates. The skill orchestrates multiple audits (code quality, security, performance, documentation) and generates deployment checklists with binary go/no-go decisions.

Use this skill as the final validation stage before ANY production deployment, whether first release or routine update. The 1-2 week investment in readiness validation saves weeks or months of incident response, customer trust erosion, and emergency fixes. Quality gates are intentionally strict - production failures are exponentially more expensive than blocked deployments.

The framework integrates with existing CI/CD pipelines as an automated gate, failing builds when readiness criteria are not met. This enforces production discipline and eliminates subjective "ship it anyway" decisions that bypass safety checks.

Success requires treating readiness validation as non-negotiable - partial passes are failures. The difference between reliable production systems and incident-prone systems is systematic adherence to quality gates, not individual developer skill.

## Core Principles (Additional)

### Principle 4: Monitoring Is Part of Readiness, Not an Afterthought
Production deployment without monitoring is blind - you cannot validate success or respond to failures without observability. Logging, metrics, alerts, and dashboards must be operational BEFORE initial deployment, not added reactively after incidents occur.

In practice:
- Health checks (readiness + liveness probes) configured for each service
- Error tracking integrated (Sentry, Rollbar) with alert thresholds
- Metrics collection enabled (response times, error rates, resource usage)
- Dashboard created showing deployment-specific KPIs

### Principle 5: Rollback Plans Must Be Tested, Not Just Documented
A rollback plan that has never been executed is theoretical at best and broken at worst. Production readiness requires validated rollback procedures with time-bounded SLAs (target: <5 minutes to previous version).

In practice:
- Rollback tested in staging environment before production deployment
- Rollback commands documented as runnable scripts (not prose instructions)
- Rollback SLA defined and measured (automated reversion triggers on health check failures)
- Blue-green or canary deployment patterns enable instant rollback

### Principle 6: Deployment Windows Are Risk Management, Not Bureaucracy
Deploying during off-peak hours with full team availability is risk mitigation, not process theater. Friday afternoon deployments without on-call coverage guarantee extended downtime when issues occur, eroding customer trust and team morale.

In practice:
- Deploy Tuesday-Wednesday during business hours (avoid Mondays for week start issues, avoid Fridays for weekend gaps)
- Full on-call team notified and available during deployment window
- Incident response runbooks prepared and accessible
- Deployment paused during major holidays, product launches, or high-traffic events

## Common Anti-Patterns (Additional)

| Anti-Pattern | Problem | Solution |
|--------------|---------|----------|
| **"Monitoring After Launch"** | Deploying to production without logging, metrics, or alerts configured. First incident is discovered by customers, not teams. No diagnostic data available for troubleshooting. | Monitoring setup is a quality gate. Block deployment if health checks undefined, error tracking missing, or metrics collection not enabled. Observability is not optional. |
| **"Rollback Is for Emergencies"** | Rollback plans exist as documentation but are never tested. When deployment fails, rollback procedure itself is broken, compounding downtime from minutes to hours. | Test rollback in staging before every production deployment. Rollback should be a routine operation, not a crisis procedure. Target: rollback completes in <5 minutes. |
| **"We Can Deploy Anytime"** | Deploying without considering deployment window risks - Friday deployments, off-peak hours with no coverage, during major customer events. Issues discovered when no one is available to respond. | Enforce deployment windows: Tuesday-Wednesday business hours, full team available, avoid holidays/launches. Production issues are inevitable - timing determines impact magnitude. |

## Conclusion (Extended)

Production Readiness provides a systematic framework for transforming functionally correct code into deployment-ready software through comprehensive validation across quality, security, performance, documentation, and operational readiness dimensions. The skill eliminates subjective "looks good" assessments by replacing them with measurable pass/fail criteria: tests 100% passing, code quality 85/100, zero critical vulnerabilities, performance within SLAs, rollback plan tested.

Use this skill as a mandatory pre-deployment gate for ALL production releases, whether initial launch or routine update. The 1-2 week investment in readiness validation prevents 90%+ of production incidents by catching issues before customer impact. Quality gates are intentionally strict - partial compliance creates the illusion of readiness while leaving vulnerabilities unaddressed. The framework enforces binary go/no-go decisions that prevent premature deployments driven by deadline pressure rather than actual preparedness.

Integration with CI/CD pipelines automates readiness enforcement, failing builds when criteria are not met and eliminating "ship it anyway" overrides. Success requires organizational discipline - production readiness is a process, not a checkbox. The difference between reliable systems and incident-prone systems is systematic adherence to quality gates, comprehensive monitoring, tested rollback plans, and strategic deployment timing. Teams that treat readiness validation as non-negotiable experience 80-90% reductions in production incidents compared to teams that deploy optimistically without validation.